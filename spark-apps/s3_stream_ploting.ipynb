{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies\n",
    "\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# %load_ext nb_black\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, count\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Vinh-Le\") \\\n",
    "        .config(\"fs.s3a.access.key\", \"${env:AWS_ACCESS_KEY_ID}\") \\\n",
    "        .config(\"fs.s3a.secret.key\", \"${env:AWS_SECRET_ACCESS_KEY}\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4') \\\n",
    "        .config('spark.jars.packages', 'org.postgresql:postgresql:42.5.4') \\\n",
    "        .config(\"fs.s3a.endpoint\", \"s3.ap-southeast-1.amazonaws.com\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"dynamic\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", 1500)\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to write the data to PostgreSQL\n",
    "def write_to_postgres(microBatchDataframe, batchId, tableName):\n",
    "    # Get the JDBC connection properties\n",
    "    jdbc_url = \"jdbc:postgresql://da-postgres:5432/mydb\"\n",
    "    jdbc_properties = {\n",
    "        \"user\": \"myuser\",\n",
    "        \"password\": \"mypassword\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Write the dataframe to PostgreSQL\n",
    "    microBatchDataframe.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(url=jdbc_url, table=tableName, properties=jdbc_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"az-id\", StringType(), True),\n",
    "    StructField(\"dstaddr\", StringType(), True),\n",
    "    StructField(\"dstport\", LongType(), True),\n",
    "    StructField(\"srcaddr\", StringType(), True),\n",
    "    StructField(\"srcport\", LongType(), True),\n",
    "    StructField(\"start\", LongType(), True),\n",
    "    StructField(\"end\", LongType(), True),\n",
    "    StructField(\"log-status\", StringType(), True),\n",
    "    StructField(\"packets\", LongType(), True),\n",
    "    StructField(\"protocol\", LongType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"subnet-id\", StringType(), True),\n",
    "    StructField(\"tcp-flags\", LongType(), True),\n",
    "    StructField(\"traffic-path\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"vpc-id\", StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark.readStream.schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"s3a://cdp-dev-env-dq/spark-streaming/AWSLogs/621074188511/vpcflowlogs/ap-southeast-1/*/*/*/\", schema=schema, header = True, sep = ' ')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col, to_date, to_timestamp, date_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_dict = {\n",
    "    '1': 'ICMP',\n",
    "    '2': 'IGMP', \n",
    "    '3': 'GGP',\n",
    "    '4': 'IP-in-IP',\n",
    "    '6': 'TCP',\n",
    "    '8': 'EGP',\n",
    "    '9': 'IGP',\n",
    "    '17': 'UDP',\n",
    "    '41': 'IPv6',\n",
    "    '47': 'GRE',\n",
    "    '50': 'ESP',\n",
    "    '51': 'AH',\n",
    "    '58': 'ICMPv6',\n",
    "    '88': 'EIGRP',\n",
    "    '89': 'OSPF',\n",
    "    '92': 'MSP',\n",
    "    '103': 'PIM',\n",
    "    '112': 'VRRP',\n",
    "    '115': 'L2TP',\n",
    "    '132': 'SCTP',\n",
    "    '136': 'UDPLite'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, floor\n",
    "\n",
    "cleaned_streaming = streaming.withColumn(\"start\", from_unixtime(\"start\").cast(\"timestamp\")) \\\n",
    "                             .withColumn(\"end\", from_unixtime(\"end\").cast(\"timestamp\")) \\\n",
    "                             .withColumn(\"protocol\", when(col(\"protocol\") == \"1\", \"ICMP\")\n",
    "                                         .when(col(\"protocol\") == \"2\", \"IGMP\")\n",
    "                                         .when(col(\"protocol\") == \"3\", \"GGP\")\n",
    "                                         .when(col(\"protocol\") == \"4\", \"IP-in-IP\")\n",
    "                                         .when(col(\"protocol\") == \"6\", \"TCP\")\n",
    "                                         .when(col(\"protocol\") == \"8\", \"EGP\")\n",
    "                                         .when(col(\"protocol\") == \"9\", \"IGP\")\n",
    "                                         .when(col(\"protocol\") == \"17\", \"UDP\")\n",
    "                                         .when(col(\"protocol\") == \"41\", \"IPv6\")\n",
    "                                         .when(col(\"protocol\") == \"47\", \"GRE\")\n",
    "                                         .when(col(\"protocol\") == \"50\", \"ESP\")\n",
    "                                         .when(col(\"protocol\") == \"51\", \"AH\")\n",
    "                                         .when(col(\"protocol\") == \"58\", \"ICMPv6\")\n",
    "                                         .when(col(\"protocol\") == \"88\", \"EIGRP\")\n",
    "                                         .when(col(\"protocol\") == \"89\", \"OSPF\")\n",
    "                                         .when(col(\"protocol\") == \"92\", \"MSP\")\n",
    "                                         .when(col(\"protocol\") == \"103\", \"PIM\")\n",
    "                                         .when(col(\"protocol\") == \"112\", \"VRRP\")\n",
    "                                         .when(col(\"protocol\") == \"115\", \"L2TP\")\n",
    "                                         .when(col(\"protocol\") == \"132\", \"SCTP\")\n",
    "                                         .when(col(\"protocol\") == \"136\", \"UDPLite\")\n",
    "                                         .otherwise(\"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, split, substring\n",
    "\n",
    "# Create the boolean conditions\n",
    "accepted_idx = (col(\"action\") == \"ACCEPT\")\n",
    "rejected_idx = (col(\"action\") == \"REJECT\")\n",
    "\n",
    "dst_ip = split(col(\"dstaddr\"), \"\\\\.\")\n",
    "src_ip = split(col(\"srcaddr\"), \"\\\\.\")\n",
    "dst_idx = (substring(col(\"dstaddr\"), 1, 4) == \"10.0\")\n",
    "src_idx = (substring(col(\"srcaddr\"), 1, 4) == \"10.0\")\n",
    "\n",
    "in_idx = ~dst_idx & src_idx\n",
    "out_idx = ~src_idx & dst_idx\n",
    "local_idx = src_idx & dst_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the boolean conditions to the DataFrame|\n",
    "indexed_streaming = cleaned_streaming.withColumn(\"accepted_idx\", accepted_idx) \\\n",
    "                                    .withColumn(\"rejected_idx\", rejected_idx) \\\n",
    "                                    .withColumn(\"out_idx\", out_idx) \\\n",
    "                                    .withColumn(\"in_idx\", in_idx) \\\n",
    "                                    .withColumn(\"local_idx\", local_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the streaming DataFrame with a watermark\n",
    "local_traffic = indexed_streaming.filter(indexed_streaming.local_idx == True) \\\n",
    "                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                .groupBy(\"start\") \\\n",
    "                .count()\n",
    "\n",
    "accepted_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.accepted_idx == True) \\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "rejected_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.rejected_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "accepted_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "\n",
    "rejected_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.rejected_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "top10_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('srcaddr', 'start').count()\n",
    "top10_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('srcaddr', 'start').count()\n",
    "\n",
    "protocol_stream = indexed_streaming.select('protocol', 'start')\\\n",
    "                                    .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                    .groupBy('protocol', 'start').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Write the stream to PostgreSQL\n",
    "protocol_query = (\n",
    "    protocol_stream.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"protocol\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/protocol/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# protocol_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Write the stream to PostgreSQL\n",
    "local_query = (\n",
    "    local_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"local\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/local/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Write the stream to PostgreSQL\n",
    "rejected_in_query = (\n",
    "    rejected_in_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"rejected_in\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/rejected_in/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Write the stream to PostgreSQL\n",
    "accepted_in_query = (\n",
    "    accepted_in_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"accepted_in\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/accepted_in/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the stream to PostgreSQL\n",
    "accepted_out_query = (\n",
    "    accepted_out_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"accepted_out\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/accepted_out/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the stream to PostgreSQL\n",
    "rejected_out_query = (\n",
    "    rejected_out_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"rejected_out\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/rejected_out/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Write the stream to PostgreSQL\n",
    "top10_in_query = (\n",
    "    top10_in_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"top10_in\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/top10_in/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Write the stream to PostgreSQL\n",
    "top10_out_query = (\n",
    "    top10_out_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"top10_out\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/top10_out/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the streaming DataFrame into a streaming DataFrame with two fields: type_of_packet and count\n",
    "packet_type_count = indexed_streaming \\\n",
    "    .withColumn(\"type_of_packet\", when(col(\"in_idx\") == \"true\", \"inbound\")\n",
    "                .when(col(\"out_idx\") == \"true\", \"outbound\")\n",
    "                .when(col(\"local_idx\") == \"true\", \"local\")\n",
    "                .otherwise(\"unknown\")) \\\n",
    "    .withWatermark(\"start\", \"1 minute\") \\\n",
    "    .groupBy(\"type_of_packet\", \"start\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .select(\"type_of_packet\", \"start\", 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the stream to PostgreSQL\n",
    "packet_type_query = (\n",
    "    packet_type_count.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"type\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/packet_type/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "# local_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 132.0 in stage 151.0 (TID 13502) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 135.0 in stage 151.0 (TID 13505) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 133.0 in stage 151.0 (TID 13503) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 134.0 in stage 151.0 (TID 13504) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 10:05:19 ERROR TaskSchedulerImpl: Lost executor 0 on 172.26.0.5: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "top10_out_query.stop()\n",
    "top10_in_query.stop()\n",
    "packet_type_query.stop()\n",
    "rejected_out_query.stop()\n",
    "accepted_in_query.stop()\n",
    "accepted_out_query.stop()\n",
    "rejected_in_query.stop()\n",
    "local_query.stop()\n",
    "protocol_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/spark/python/pyspark/sql/utils.py\", line 120, in call\\n    raise e\\n  File \"/opt/spark/python/pyspark/sql/utils.py\", line 117, in call\\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\\n  File \"/tmp/ipykernel_3403/723357393.py\", line 5, in <lambda>\\n    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"type\"))\\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/tmp/ipykernel_3403/1940519892.py\", line 14, in write_to_postgres\\n    .jdbc(url=jdbc_url, table=tableName, properties=jdbc_properties)\\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/spark/python/pyspark/sql/readwriter.py\", line 1984, in jdbc\\n    self.mode(mode)._jwrite.jdbc(url, table, jprop)\\n  File \"/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\\n    return_value = get_return_value(\\n                   ^^^^^^^^^^^^^^^^^\\n  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\\n    return f(*a, **kw)\\n           ^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling o1764.jdbc.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 171.0 failed 4 times, most recent failure: Lost task 0.3 in stage 171.0 (TID 15339) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO type (\"type_of_packet\",\"start\",\"count\") VALUES (\\'unknown\\',\\'2024-06-07 10:29:17+00\\'::timestamp,2) was aborted: ERROR: duplicate key value violates unique constraint \"type_pkey\"\\n  Detail: Key (start, type_of_packet)=(2024-06-07 10:29:17, unknown) already exists.  Call getNextException to see other errors in the batch.\\n\\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\\n\\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\\n\\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\\n\\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\\n\\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"type_pkey\"\\n  Detail: Key (start, type_of_packet)=(2024-06-07 10:29:17, unknown) already exists.\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\\n\\t... 21 more\\n\\nDriver stacktrace:\\n\\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\\n\\tat scala.Option.foreach(Option.scala:407)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\\n\\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\\n\\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\\n\\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\\n\\tat com.sun.proxy.$Proxy36.call(Unknown Source)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\\n\\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\\n\\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO type (\"type_of_packet\",\"start\",\"count\") VALUES (\\'unknown\\',\\'2024-06-07 10:29:17+00\\'::timestamp,2) was aborted: ERROR: duplicate key value violates unique constraint \"type_pkey\"\\n  Detail: Key (start, type_of_packet)=(2024-06-07 10:29:17, unknown) already exists.  Call getNextException to see other errors in the batch.\\n\\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\\n\\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\\n\\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\\n\\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\\n\\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"type_pkey\"\\n  Detail: Key (start, type_of_packet)=(2024-06-07 10:29:17, unknown) already exists.\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\\n\\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\\n\\t... 21 more\\n\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:38:43 WARN TaskSetManager: Lost task 61.0 in stage 201.0 (TID 18059) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO rejected_out (\"start\",\"count\") VALUES ('2024-06-07 10:32:12+00',16) was aborted: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "24/06/07 10:38:44 ERROR TaskSetManager: Task 61 in stage 201.0 failed 4 times; aborting job\n",
      "24/06/07 10:38:44 WARN TaskSetManager: Lost task 84.0 in stage 201.0 (TID 18085) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 61 in stage 201.0 failed 4 times, most recent failure: Lost task 61.3 in stage 201.0 (TID 18081) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO rejected_out (\"start\",\"count\") VALUES ('2024-06-07 10:32:12+00',16) was aborted: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:44 WARN TaskSetManager: Lost task 82.0 in stage 201.0 (TID 18083) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 61 in stage 201.0 failed 4 times, most recent failure: Lost task 61.3 in stage 201.0 (TID 18081) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO rejected_out (\"start\",\"count\") VALUES ('2024-06-07 10:32:12+00',16) was aborted: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:44 WARN TaskSetManager: Lost task 83.0 in stage 201.0 (TID 18084) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 61 in stage 201.0 failed 4 times, most recent failure: Lost task 61.3 in stage 201.0 (TID 18081) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO rejected_out (\"start\",\"count\") VALUES ('2024-06-07 10:32:12+00',16) was aborted: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:44 WARN TaskSetManager: Lost task 81.0 in stage 201.0 (TID 18082) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 61 in stage 201.0 failed 4 times, most recent failure: Lost task 61.3 in stage 201.0 (TID 18081) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO rejected_out (\"start\",\"count\") VALUES ('2024-06-07 10:32:12+00',16) was aborted: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:44 ERROR MicroBatchExecution: Query [id = 67b181c7-e7ac-48eb-bf71-a3647d9dcc5a, runId = 10d38ce8-26f9-4e4d-81ad-918d60f64df9] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_3403/1986478025.py\", line 5, in <lambda>\n",
      "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"rejected_out\"))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3403/1940519892.py\", line 14, in write_to_postgres\n",
      "    .jdbc(url=jdbc_url, table=tableName, properties=jdbc_properties)\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/sql/readwriter.py\", line 1984, in jdbc\n",
      "    self.mode(mode)._jwrite.jdbc(url, table, jprop)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1910.jdbc.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 61 in stage 201.0 failed 4 times, most recent failure: Lost task 61.3 in stage 201.0 (TID 18081) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO rejected_out (\"start\",\"count\") VALUES ('2024-06-07 10:32:12+00',16) was aborted: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor189.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO rejected_out (\"start\",\"count\") VALUES ('2024-06-07 10:32:12+00',16) was aborted: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"rejected_out_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:12) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/06/07 10:38:49 WARN TaskSetManager: Lost task 12.0 in stage 205.0 (TID 18298) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO accepted_in (\"start\",\"count\") VALUES ('2024-06-07 10:32:40+00',33) was aborted: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "24/06/07 10:38:49 ERROR TaskSetManager: Task 12 in stage 205.0 failed 4 times; aborting job\n",
      "24/06/07 10:38:49 WARN TaskSetManager: Lost task 27.0 in stage 205.0 (TID 18316) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 12 in stage 205.0 failed 4 times, most recent failure: Lost task 12.3 in stage 205.0 (TID 18314) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO accepted_in (\"start\",\"count\") VALUES ('2024-06-07 10:32:40+00',33) was aborted: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:49 WARN TaskSetManager: Lost task 28.0 in stage 205.0 (TID 18317) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 12 in stage 205.0 failed 4 times, most recent failure: Lost task 12.3 in stage 205.0 (TID 18314) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO accepted_in (\"start\",\"count\") VALUES ('2024-06-07 10:32:40+00',33) was aborted: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:49 WARN TaskSetManager: Lost task 29.0 in stage 205.0 (TID 18318) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 12 in stage 205.0 failed 4 times, most recent failure: Lost task 12.3 in stage 205.0 (TID 18314) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO accepted_in (\"start\",\"count\") VALUES ('2024-06-07 10:32:40+00',33) was aborted: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:49 WARN TaskSetManager: Lost task 26.0 in stage 205.0 (TID 18315) (172.26.0.5 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 12 in stage 205.0 failed 4 times, most recent failure: Lost task 12.3 in stage 205.0 (TID 18314) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO accepted_in (\"start\",\"count\") VALUES ('2024-06-07 10:32:40+00',33) was aborted: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/06/07 10:38:49 ERROR MicroBatchExecution: Query [id = db14dd3a-d243-49af-8c9f-d2b140021095, runId = 90b24fa1-f340-4f86-a5ad-3289a3caa0d1] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_3403/1318567160.py\", line 5, in <lambda>\n",
      "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"accepted_in\"))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3403/1940519892.py\", line 14, in write_to_postgres\n",
      "    .jdbc(url=jdbc_url, table=tableName, properties=jdbc_properties)\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/sql/readwriter.py\", line 1984, in jdbc\n",
      "    self.mode(mode)._jwrite.jdbc(url, table, jprop)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1917.jdbc.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 205.0 failed 4 times, most recent failure: Lost task 12.3 in stage 205.0 (TID 18314) (172.26.0.5 executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO accepted_in (\"start\",\"count\") VALUES ('2024-06-07 10:32:40+00',33) was aborted: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor189.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO accepted_in (\"start\",\"count\") VALUES ('2024-06-07 10:32:40+00',33) was aborted: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"accepted_in_pkey\"\n",
      "  Detail: Key (start)=(2024-06-07 10:32:40) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 21 more\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "[Stage 207:(56 + 4) / 200][Stage 209:>(0 + 0) / 200][Stage 210:>  (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "packet_type_query.status['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:39:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6f1295db-7525-47b7-9f15-4fdaf564646a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/07 10:39:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "|     TCP|2024-06-07 10:28:05|    2|\n",
      "|     TCP|2024-06-07 10:29:19|    6|\n",
      "|     UDP|2024-06-07 10:28:36|    1|\n",
      "|     TCP|2024-06-07 10:29:26|   34|\n",
      "| Unknown|2024-06-07 10:28:57|    1|\n",
      "|     TCP|2024-06-07 10:27:44|    3|\n",
      "|     TCP|2024-06-07 10:29:29|    2|\n",
      "| Unknown|2024-06-07 10:27:52|    1|\n",
      "| Unknown|2024-06-07 10:28:20|    1|\n",
      "| Unknown|2024-06-07 10:27:24|    2|\n",
      "| Unknown|2024-06-07 10:27:09|    1|\n",
      "| Unknown|2024-06-07 10:28:13|    1|\n",
      "| Unknown|2024-06-07 10:28:34|    4|\n",
      "|     TCP|2024-06-07 10:29:33|   30|\n",
      "| Unknown|2024-06-07 10:27:06|    3|\n",
      "|     TCP|2024-06-07 10:29:13|    1|\n",
      "|     TCP|2024-06-07 10:29:27|   29|\n",
      "|     TCP|2024-06-07 10:28:57|   60|\n",
      "| Unknown|2024-06-07 10:27:28|    1|\n",
      "| Unknown|2024-06-07 10:28:52|    1|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "| Unknown|2024-06-07 10:30:36|    1|\n",
      "| Unknown|2024-06-07 10:31:56|    1|\n",
      "|     UDP|2024-06-07 10:32:27|    2|\n",
      "| Unknown|2024-06-07 10:28:57|    2|\n",
      "|     TCP|2024-06-07 10:30:39|   12|\n",
      "| Unknown|2024-06-07 10:31:20|    2|\n",
      "| Unknown|2024-06-07 10:30:55|    3|\n",
      "|     TCP|2024-06-07 10:30:19|    8|\n",
      "|     TCP|2024-06-07 10:32:29|    6|\n",
      "| Unknown|2024-06-07 10:31:28|    4|\n",
      "| Unknown|2024-06-07 10:30:16|    2|\n",
      "|     UDP|2024-06-07 10:30:40|    2|\n",
      "|     UDP|2024-06-07 10:31:12|    2|\n",
      "| Unknown|2024-06-07 10:29:44|    3|\n",
      "|    ICMP|2024-06-07 10:30:04|    1|\n",
      "|     TCP|2024-06-07 10:32:10|   73|\n",
      "| Unknown|2024-06-07 10:29:42|    1|\n",
      "| Unknown|2024-06-07 10:30:29|    1|\n",
      "| Unknown|2024-06-07 10:32:17|    2|\n",
      "| Unknown|2024-06-07 10:29:35|    1|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "|    IPv6|2024-06-07 10:33:43|    1|\n",
      "| Unknown|2024-06-07 10:33:26|    2|\n",
      "| Unknown|2024-06-07 10:32:07|    1|\n",
      "|     TCP|2024-06-07 10:33:46|   27|\n",
      "|    ICMP|2024-06-07 10:34:28|    1|\n",
      "| Unknown|2024-06-07 10:32:39|    4|\n",
      "|     TCP|2024-06-07 10:33:14|    8|\n",
      "|     TCP|2024-06-07 10:31:56|   21|\n",
      "| Unknown|2024-06-07 10:33:25|    2|\n",
      "| Unknown|2024-06-07 10:34:21|    1|\n",
      "|     TCP|2024-06-07 10:32:10|   80|\n",
      "|     TCP|2024-06-07 10:33:12|   12|\n",
      "|     TCP|2024-06-07 10:33:45|   26|\n",
      "| Unknown|2024-06-07 10:34:05|    2|\n",
      "| Unknown|2024-06-07 10:34:02|    2|\n",
      "|     TCP|2024-06-07 10:34:48|    1|\n",
      "|     UDP|2024-06-07 10:34:41|    1|\n",
      "|     TCP|2024-06-07 10:32:53|    7|\n",
      "|     TCP|2024-06-07 10:33:35|   11|\n",
      "|     TCP|2024-06-07 10:33:29|    9|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "|     TCP|2024-06-07 10:34:57|   25|\n",
      "|    ICMP|2024-06-07 10:35:36|    2|\n",
      "|    IPv6|2024-06-07 10:35:49|    1|\n",
      "|     TCP|2024-06-07 10:37:20|    2|\n",
      "| Unknown|2024-06-07 10:35:40|    1|\n",
      "|     UDP|2024-06-07 10:34:41|    3|\n",
      "|     TCP|2024-06-07 10:35:13|   18|\n",
      "|     TCP|2024-06-07 10:35:16|   23|\n",
      "| Unknown|2024-06-07 10:35:18|    3|\n",
      "| Unknown|2024-06-07 10:34:38|    1|\n",
      "|     TCP|2024-06-07 10:35:51|    2|\n",
      "|     TCP|2024-06-07 10:35:54|   21|\n",
      "| Unknown|2024-06-07 10:36:17|    3|\n",
      "| Unknown|2024-06-07 10:36:30|    1|\n",
      "|     TCP|2024-06-07 10:35:47|   33|\n",
      "|     TCP|2024-06-07 10:37:37|    1|\n",
      "|     UDP|2024-06-07 10:37:40|    2|\n",
      "| Unknown|2024-06-07 10:35:53|    1|\n",
      "|     TCP|2024-06-07 10:37:42|   29|\n",
      "| Unknown|2024-06-07 10:34:55|    2|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------+-----+-----+\n",
      "|protocol|start|count|\n",
      "+--------+-----+-----+\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:42:24 WARN FileStreamSource: Listed 4 file(s) in 5282 ms\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "| Unknown|2024-06-07 10:37:16|    1|\n",
      "| Unknown|2024-06-07 10:38:07|    1|\n",
      "|     TCP|2024-06-07 10:38:16|   43|\n",
      "|     TCP|2024-06-07 10:39:33|    1|\n",
      "| Unknown|2024-06-07 10:38:43|    3|\n",
      "| Unknown|2024-06-07 10:38:57|    2|\n",
      "|     UDP|2024-06-07 10:38:28|    2|\n",
      "| Unknown|2024-06-07 10:38:34|    2|\n",
      "| Unknown|2024-06-07 10:39:05|    2|\n",
      "| Unknown|2024-06-07 10:37:01|    1|\n",
      "|     TCP|2024-06-07 10:38:37|   23|\n",
      "| Unknown|2024-06-07 10:39:00|    2|\n",
      "| Unknown|2024-06-07 10:38:32|    2|\n",
      "|     UDP|2024-06-07 10:38:41|    2|\n",
      "|     UDP|2024-06-07 10:38:56|    3|\n",
      "| Unknown|2024-06-07 10:39:21|    1|\n",
      "| Unknown|2024-06-07 10:37:47|    1|\n",
      "|     TCP|2024-06-07 10:38:51|    3|\n",
      "|     UDP|2024-06-07 10:39:00|    1|\n",
      "| Unknown|2024-06-07 10:37:27|    3|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "|    ICMP|2024-06-07 10:39:53|    1|\n",
      "| Unknown|2024-06-07 10:39:13|    2|\n",
      "|     TCP|2024-06-07 10:40:44|    5|\n",
      "| Unknown|2024-06-07 10:41:00|    1|\n",
      "| Unknown|2024-06-07 10:41:29|    1|\n",
      "| Unknown|2024-06-07 10:38:57|    4|\n",
      "|     UDP|2024-06-07 10:39:41|    2|\n",
      "| Unknown|2024-06-07 10:42:03|    1|\n",
      "| Unknown|2024-06-07 10:39:05|    3|\n",
      "|     TCP|2024-06-07 10:40:09|    7|\n",
      "|     UDP|2024-06-07 10:40:20|    2|\n",
      "|     UDP|2024-06-07 10:41:16|    2|\n",
      "|     TCP|2024-06-07 10:40:18|   14|\n",
      "| Unknown|2024-06-07 10:41:26|    2|\n",
      "| Unknown|2024-06-07 10:42:07|    1|\n",
      "|     TCP|2024-06-07 10:40:07|   10|\n",
      "|     TCP|2024-06-07 10:40:59|    6|\n",
      "| Unknown|2024-06-07 10:40:37|    1|\n",
      "|     TCP|2024-06-07 10:41:45|    9|\n",
      "|     TCP|2024-06-07 10:42:07|    9|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------+-----+-----+\n",
      "|protocol|start|count|\n",
      "+--------+-----+-----+\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 261:(187 + 4) / 200][Stage 263:>(0 + 0) / 200][Stage 265:>(0 + 0) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "|    ICMP|2024-06-07 10:42:47|    1|\n",
      "|     TCP|2024-06-07 10:43:30|    2|\n",
      "| Unknown|2024-06-07 10:42:44|    2|\n",
      "|     UDP|2024-06-07 10:42:56|    2|\n",
      "| Unknown|2024-06-07 10:43:20|    2|\n",
      "|     UDP|2024-06-07 10:43:13|    2|\n",
      "| Unknown|2024-06-07 10:42:10|    1|\n",
      "|     TCP|2024-06-07 10:43:08|   11|\n",
      "| Unknown|2024-06-07 10:42:07|    4|\n",
      "|     TCP|2024-06-07 10:42:07|   20|\n",
      "| Unknown|2024-06-07 10:42:30|    1|\n",
      "|     TCP|2024-06-07 10:42:14|   13|\n",
      "|     UDP|2024-06-07 10:43:41|    2|\n",
      "|    IPv6|2024-06-07 10:43:08|    1|\n",
      "| Unknown|2024-06-07 10:42:26|    3|\n",
      "| Unknown|2024-06-07 10:44:05|    1|\n",
      "|     UDP|2024-06-07 10:44:52|    1|\n",
      "| Unknown|2024-06-07 10:43:08|    5|\n",
      "| Unknown|2024-06-07 10:42:54|    3|\n",
      "|     TCP|2024-06-07 10:43:57|   46|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "| Unknown|2024-06-07 10:47:08|    3|\n",
      "| Unknown|2024-06-07 10:45:09|    3|\n",
      "|     TCP|2024-06-07 10:47:17|    2|\n",
      "| Unknown|2024-06-07 10:44:44|    4|\n",
      "|     TCP|2024-06-07 10:45:29|   39|\n",
      "|     TCP|2024-06-07 10:46:49|    2|\n",
      "|     TCP|2024-06-07 10:44:40|   77|\n",
      "|     TCP|2024-06-07 10:45:40|   11|\n",
      "|     TCP|2024-06-07 10:45:32|   34|\n",
      "|     UDP|2024-06-07 10:46:07|    2|\n",
      "|     TCP|2024-06-07 10:45:15|   24|\n",
      "| Unknown|2024-06-07 10:45:12|    2|\n",
      "|     TCP|2024-06-07 10:44:43|   28|\n",
      "|     TCP|2024-06-07 10:46:46|   28|\n",
      "| Unknown|2024-06-07 10:46:04|    4|\n",
      "| Unknown|2024-06-07 10:46:10|    2|\n",
      "| Unknown|2024-06-07 10:45:18|    3|\n",
      "|     TCP|2024-06-07 10:45:28|   34|\n",
      "|     TCP|2024-06-07 10:46:45|   33|\n",
      "|     TCP|2024-06-07 10:47:00|    1|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+--------+-----+-----+\n",
      "|protocol|start|count|\n",
      "+--------+-----+-----+\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "| Unknown|2024-06-07 10:47:14|    1|\n",
      "|     TCP|2024-06-07 10:48:08|    7|\n",
      "|     TCP|2024-06-07 10:49:03|    1|\n",
      "| Unknown|2024-06-07 10:48:31|    1|\n",
      "|    ICMP|2024-06-07 10:49:18|    1|\n",
      "| Unknown|2024-06-07 10:47:32|    1|\n",
      "| Unknown|2024-06-07 10:47:34|    1|\n",
      "|     TCP|2024-06-07 10:48:53|    1|\n",
      "| Unknown|2024-06-07 10:49:08|    2|\n",
      "|     TCP|2024-06-07 10:47:53|    2|\n",
      "| Unknown|2024-06-07 10:48:38|    1|\n",
      "| Unknown|2024-06-07 10:49:21|    2|\n",
      "| Unknown|2024-06-07 10:49:18|    3|\n",
      "|     TCP|2024-06-07 10:49:41|    3|\n",
      "| Unknown|2024-06-07 10:47:44|    3|\n",
      "|     TCP|2024-06-07 10:48:04|    1|\n",
      "| Unknown|2024-06-07 10:49:25|    1|\n",
      "| Unknown|2024-06-07 10:47:28|    2|\n",
      "|     TCP|2024-06-07 10:48:36|   24|\n",
      "|     TCP|2024-06-07 10:47:42|    4|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "| Unknown|2024-06-07 10:50:00|    1|\n",
      "|     TCP|2024-06-07 10:52:16|   28|\n",
      "|     TCP|2024-06-07 10:52:42|   63|\n",
      "| Unknown|2024-06-07 10:50:02|    1|\n",
      "|     TCP|2024-06-07 10:50:56|    3|\n",
      "| Unknown|2024-06-07 10:51:10|    3|\n",
      "| Unknown|2024-06-07 10:49:34|    2|\n",
      "|     TCP|2024-06-07 10:51:31|    8|\n",
      "| Unknown|2024-06-07 10:51:38|    1|\n",
      "| Unknown|2024-06-07 10:50:07|    1|\n",
      "|     TCP|2024-06-07 10:50:32|    2|\n",
      "|     TCP|2024-06-07 10:52:02|    4|\n",
      "| Unknown|2024-06-07 10:50:13|    3|\n",
      "| Unknown|2024-06-07 10:51:52|    1|\n",
      "|     TCP|2024-06-07 10:52:48|   13|\n",
      "| Unknown|2024-06-07 10:50:56|    2|\n",
      "| Unknown|2024-06-07 10:52:13|    1|\n",
      "| Unknown|2024-06-07 10:50:09|    5|\n",
      "| Unknown|2024-06-07 10:49:08|    3|\n",
      "|     TCP|2024-06-07 10:51:25|    2|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+--------+-----+-----+\n",
      "|protocol|start|count|\n",
      "+--------+-----+-----+\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "| Unknown|2024-06-07 10:53:43|    1|\n",
      "| Unknown|2024-06-07 10:54:53|    1|\n",
      "| Unknown|2024-06-07 10:54:44|    3|\n",
      "| Unknown|2024-06-07 10:56:05|    1|\n",
      "|     UDP|2024-06-07 10:57:20|    3|\n",
      "|     TCP|2024-06-07 10:55:02|   11|\n",
      "| Unknown|2024-06-07 10:54:14|    1|\n",
      "|     TCP|2024-06-07 10:56:14|    6|\n",
      "|     TCP|2024-06-07 10:56:32|    9|\n",
      "| Unknown|2024-06-07 10:57:00|    1|\n",
      "|     UDP|2024-06-07 10:55:17|    2|\n",
      "| Unknown|2024-06-07 10:54:08|    3|\n",
      "| Unknown|2024-06-07 10:55:01|    2|\n",
      "|     TCP|2024-06-07 10:55:20|    1|\n",
      "|    ICMP|2024-06-07 10:56:32|    1|\n",
      "|     TCP|2024-06-07 10:55:27|   34|\n",
      "| Unknown|2024-06-07 10:53:50|    1|\n",
      "|     TCP|2024-06-07 10:55:28|   33|\n",
      "| Unknown|2024-06-07 10:55:43|    4|\n",
      "|     TCP|2024-06-07 10:55:07|    8|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-----+\n",
      "|protocol|              start|count|\n",
      "+--------+-------------------+-----+\n",
      "|     TCP|2024-06-07 10:52:42|   65|\n",
      "| Unknown|2024-06-07 10:53:43|    4|\n",
      "|     TCP|2024-06-07 10:53:53|   13|\n",
      "|    ICMP|2024-06-07 10:53:46|    1|\n",
      "| Unknown|2024-06-07 10:53:32|    3|\n",
      "| Unknown|2024-06-07 10:53:31|    1|\n",
      "|     TCP|2024-06-07 10:53:10|   74|\n",
      "| Unknown|2024-06-07 10:52:56|    1|\n",
      "|     UDP|2024-06-07 10:53:07|    1|\n",
      "| Unknown|2024-06-07 10:54:17|    1|\n",
      "|     TCP|2024-06-07 10:54:32|    4|\n",
      "|    ICMP|2024-06-07 10:53:38|    2|\n",
      "| Unknown|2024-06-07 10:54:08|    5|\n",
      "|     TCP|2024-06-07 10:54:17|    5|\n",
      "|     TCP|2024-06-07 10:52:48|   34|\n",
      "| Unknown|2024-06-07 10:53:50|    2|\n",
      "| Unknown|2024-06-07 10:52:35|    1|\n",
      "|     TCP|2024-06-07 10:53:44|   22|\n",
      "| Unknown|2024-06-07 10:54:16|    1|\n",
      "|     UDP|2024-06-07 10:53:32|    1|\n",
      "+--------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "protocol_query = (\n",
    "    protocol_stream.writeStream.queryName(\"protocol_counts\")\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

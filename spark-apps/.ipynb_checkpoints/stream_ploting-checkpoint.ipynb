{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.52.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.1/302.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.52.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_aarch64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m435.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.52.4 kiwisolver-1.4.5 matplotlib-3.9.0 pillow-10.3.0 pyparsing-3.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, count\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/31 02:52:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"az-id\", StringType(), True),\n",
    "    StructField(\"dstaddr\", StringType(), True),\n",
    "    StructField(\"dstport\", LongType(), True),\n",
    "    StructField(\"srcaddr\", StringType(), True),\n",
    "    StructField(\"srcport\", LongType(), True),\n",
    "    StructField(\"start\", LongType(), True),\n",
    "    StructField(\"end\", LongType(), True),\n",
    "    StructField(\"log-status\", StringType(), True),\n",
    "    StructField(\"packets\", LongType(), True),\n",
    "    StructField(\"protocol\", LongType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"subnet-id\", StringType(), True),\n",
    "    StructField(\"tcp-flags\", LongType(), True),\n",
    "    StructField(\"traffic-path\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"vpc-id\", StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark.readStream.schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"data/vpclog/\", schema=schema, header = True, sep = ' ')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col, to_date, to_timestamp, date_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_dict = {\n",
    "    '1': 'ICMP',\n",
    "    '2': 'IGMP', \n",
    "    '3': 'GGP',\n",
    "    '4': 'IP-in-IP',\n",
    "    '6': 'TCP',\n",
    "    '8': 'EGP',\n",
    "    '9': 'IGP',\n",
    "    '17': 'UDP',\n",
    "    '41': 'IPv6',\n",
    "    '47': 'GRE',\n",
    "    '50': 'ESP',\n",
    "    '51': 'AH',\n",
    "    '58': 'ICMPv6',\n",
    "    '88': 'EIGRP',\n",
    "    '89': 'OSPF',\n",
    "    '92': 'MSP',\n",
    "    '103': 'PIM',\n",
    "    '112': 'VRRP',\n",
    "    '115': 'L2TP',\n",
    "    '132': 'SCTP',\n",
    "    '136': 'UDPLite'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Convert the 'start' and 'end' columns to timestamps\n",
    "cleaned_streaming = streaming.withColumn(\"start\", from_unixtime(\"start\").cast(\"timestamp\")) \\\n",
    "                             .withColumn(\"end\", from_unixtime(\"end\").cast(\"timestamp\"))\\\n",
    "                             .withColumn(\"protocol\", \n",
    "                                         when(col(\"protocol\") == \"1\", \"ICMP\")\n",
    "                                         .when(col(\"protocol\") == \"2\", \"IGMP\")\n",
    "                                         .when(col(\"protocol\") == \"3\", \"GGP\")\n",
    "                                         .when(col(\"protocol\") == \"4\", \"IP-in-IP\")\n",
    "                                         .when(col(\"protocol\") == \"6\", \"TCP\")\n",
    "                                         .when(col(\"protocol\") == \"8\", \"EGP\")\n",
    "                                         .when(col(\"protocol\") == \"9\", \"IGP\")\n",
    "                                         .when(col(\"protocol\") == \"17\", \"UDP\")\n",
    "                                         .when(col(\"protocol\") == \"41\", \"IPv6\")\n",
    "                                         .when(col(\"protocol\") == \"47\", \"GRE\")\n",
    "                                         .when(col(\"protocol\") == \"50\", \"ESP\")\n",
    "                                         .when(col(\"protocol\") == \"51\", \"AH\")\n",
    "                                         .when(col(\"protocol\") == \"58\", \"ICMPv6\")\n",
    "                                         .when(col(\"protocol\") == \"88\", \"EIGRP\")\n",
    "                                         .when(col(\"protocol\") == \"89\", \"OSPF\")\n",
    "                                         .when(col(\"protocol\") == \"92\", \"MSP\")\n",
    "                                         .when(col(\"protocol\") == \"103\", \"PIM\")\n",
    "                                         .when(col(\"protocol\") == \"112\", \"VRRP\")\n",
    "                                         .when(col(\"protocol\") == \"115\", \"L2TP\")\n",
    "                                         .when(col(\"protocol\") == \"132\", \"SCTP\")\n",
    "                                         .when(col(\"protocol\") == \"136\", \"UDPLite\")\n",
    "                                         .otherwise(\"Unknown\"))\n",
    "\n",
    "# Extract the date and time components\n",
    "cleaned_streaming = cleaned_streaming.withColumn(\"start_date\", to_date(\"start\")) \\\n",
    "                                    .withColumn(\"end_date\", to_date(\"end\")) \\\n",
    "                                    .withColumn(\"start_time\", date_format(\"start\", \"HH:mm:ss\")) \\\n",
    "                                    .withColumn(\"end_time\", date_format(\"end\", \"HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, split, substring\n",
    "\n",
    "# Create the boolean conditions\n",
    "accepted_idx = (col(\"action\") == \"ACCEPT\")\n",
    "rejected_idx = (col(\"action\") == \"REJECT\")\n",
    "\n",
    "dst_ip = split(col(\"dstaddr\"), \"\\\\.\")\n",
    "src_ip = split(col(\"srcaddr\"), \"\\\\.\")\n",
    "dst_idx = (substring(col(\"dstaddr\"), 1, 4) == \"10.0\")\n",
    "src_idx = (substring(col(\"srcaddr\"), 1, 4) == \"10.0\")\n",
    "\n",
    "in_idx = ~dst_idx & src_idx\n",
    "out_idx = ~src_idx & dst_idx\n",
    "local_idx = src_idx & dst_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the boolean conditions to the DataFrame|\n",
    "indexed_streaming = cleaned_streaming.withColumn(\"accepted_idx\", accepted_idx) \\\n",
    "                                    .withColumn(\"rejected_idx\", rejected_idx) \\\n",
    "                                    .withColumn(\"out_idx\", out_idx) \\\n",
    "                                    .withColumn(\"in_idx\", in_idx) \\\n",
    "                                    .withColumn(\"local_idx\", local_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the streaming DataFrame with a watermark\n",
    "local_traffic = indexed_streaming.filter(indexed_streaming.local_idx == True) \\\n",
    "                .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                .groupBy(\"timestamp\", \"start_time\") \\\n",
    "                .count()\n",
    "\n",
    "accepted_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.accepted_idx == True) \\\n",
    "                                                .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                                                .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                                                .groupBy('start_time', 'timestamp').count()\n",
    "\n",
    "rejected_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.rejected_idx == True)\\\n",
    "                                                .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                                                .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                                                .groupBy('start_time', 'timestamp').count()\n",
    "\n",
    "accepted_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                                                .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                                                .groupBy('start_time', 'timestamp').count()\n",
    "\n",
    "\n",
    "rejected_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.rejected_idx == True)\\\n",
    "                                                .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                                                .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                                                .groupBy('start_time', 'timestamp').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                                                .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                                                .groupBy('srcaddr', 'timestamp').count()\n",
    "top10_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                                                .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                                                .groupBy('srcaddr', 'timestamp').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_stream = indexed_streaming.select('protocol')\\\n",
    "                                    .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "                                    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                                    .groupBy('protocol', 'timestamp').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:52:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# protocol_query = (\n",
    "#     protocol_stream.writeStream.queryName(\"protocol_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "protocol_query = (\n",
    "    protocol_stream.writeStream\n",
    "    .queryName(\"protocol_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/protocol/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/protocol/checkpoint/\")\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# protocol_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:53:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# local_query = (\n",
    "#     local_traffic.writeStream.queryName(\"local_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "local_query = (\n",
    "    local_traffic.writeStream\n",
    "    .queryName(\"local_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/local/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/local/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_query.awaitTermination()\n",
    "local_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:54:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# accepted_in_query = (\n",
    "#     accepted_in_traffic.writeStream.queryName(\"accepted_in_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "accepted_in_query = (\n",
    "    accepted_in_traffic.writeStream\n",
    "    .queryName(\"accepted_in_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/accepted_in/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/accepted_in/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_in_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:55:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# rejected_in_query = (\n",
    "#     rejected_in_traffic.writeStream.queryName(\"rejected_in_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "rejected_in_query = (\n",
    "    rejected_in_traffic.writeStream\n",
    "    .queryName(\"rejected_in_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/rejected_in/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/rejected_in/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:56:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# accepted_out_query = (\n",
    "#     accepted_out_traffic.writeStream.queryName(\"accepted_out_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "accepted_out_query = (\n",
    "    accepted_out_traffic.writeStream\n",
    "    .queryName(\"accepted_out_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/accepted_out/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/accepted_out/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:57:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# rejected_out_query = (\n",
    "#     rejected_out_traffic.writeStream.queryName(\"rejected_out_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "rejected_out_query = (\n",
    "    rejected_out_traffic.writeStream\n",
    "    .queryName(\"rejected_out_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/rejected_out/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/rejected_out/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_in_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_out_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:58:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# top10_in_query = (\n",
    "#     top10_in_traffic.writeStream.queryName(\"top10_in_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "top10_in_query = (\n",
    "    top10_in_traffic.writeStream\n",
    "    .queryName(\"top10_in_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/top10_in/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/top10_in/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_in_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 03:58:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# top10_out_query = (\n",
    "#     top10_out_traffic.writeStream.queryName(\"top10_out_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "top10_out_query = (\n",
    "    top10_out_traffic.writeStream\n",
    "    .queryName(\"top10_out_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/top10_out/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/top10_out/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_out_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the streaming DataFrame into a streaming DataFrame with two fields: type_of_packet and count\n",
    "packet_type_count = indexed_streaming \\\n",
    "    .withColumn(\"type_of_packet\", when(col(\"in_idx\") == \"true\", \"inbound\")\n",
    "                .when(col(\"out_idx\") == \"true\", \"outbound\")\n",
    "                .when(col(\"local_idx\") == \"true\", \"local\")\n",
    "                .otherwise(\"unknown\")) \\\n",
    "    .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\"type_of_packet\", \"timestamp\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .select(\"type_of_packet\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 04:04:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# packet_type_query = (\n",
    "#     packet_type_count.writeStream.queryName(\"packet_type_counts\")\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Save the streaming DataFrame to a CSV file\n",
    "packet_type_query = (\n",
    "    packet_type_count.writeStream\n",
    "    .queryName(\"packet_type_counts\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"streaming/packet_type/data\")\n",
    "    .option(\"checkpointLocation\", \"streaming/packet_type/checkpoint/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_type_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_dict = {\n",
    "    '1': 'ICMP',\n",
    "    '2': 'IGMP', \n",
    "    '3': 'GGP',\n",
    "    '4': 'IP-in-IP',\n",
    "    '6': 'TCP',\n",
    "    '8': 'EGP',\n",
    "    '9': 'IGP',\n",
    "    '17': 'UDP',\n",
    "    '41': 'IPv6',\n",
    "    '47': 'GRE',\n",
    "    '50': 'ESP',\n",
    "    '51': 'AH',\n",
    "    '58': 'ICMPv6',\n",
    "    '88': 'EIGRP',\n",
    "    '89': 'OSPF',\n",
    "    '92': 'MSP',\n",
    "    '103': 'PIM',\n",
    "    '112': 'VRRP',\n",
    "    '115': 'L2TP',\n",
    "    '132': 'SCTP',\n",
    "    '136': 'UDPLite'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

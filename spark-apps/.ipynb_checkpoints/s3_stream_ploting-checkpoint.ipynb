{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies\n",
    "\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# %load_ext nb_black\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, count\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine\n",
    "# spark = SparkSession.builder \\\n",
    "#         .appName(\"Vinh-Le\") \\\n",
    "#         .config(\"fs.s3a.access.key\", \"AKIA6GBMDXCNWXUHCSHC\") \\\n",
    "#         .config(\"fs.s3a.secret.key\", \"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\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#         .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "#         .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4') \\\n",
    "#         .config(\"fs.s3a.endpoint\", \"s3.ap-southeast-2.amazonaws.com\") \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Vinh-Le\") \\\n",
    "        .config(\"fs.s3a.access.key\", \"AKIAZBGXIBDPZ2FJZYF5\") \\\n",
    "        .config(\"fs.s3a.secret.key\", \"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\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4') \\\n",
    "        .config('spark.jars.packages', 'org.postgresql:postgresql:42.5.4') \\\n",
    "        .config(\"fs.s3a.endpoint\", \"s3.ap-southeast-1.amazonaws.com\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"dynamic\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", 1500)\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to write the data to PostgreSQL\n",
    "def write_to_postgres(microBatchDataframe, batchId, tableName):\n",
    "    # Get the JDBC connection properties\n",
    "    jdbc_url = \"jdbc:postgresql://da-postgres:5432/mydb\"\n",
    "    jdbc_properties = {\n",
    "        \"user\": \"myuser\",\n",
    "        \"password\": \"mypassword\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Write the dataframe to PostgreSQL\n",
    "    microBatchDataframe.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(url=jdbc_url, table=tableName, properties=jdbc_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"az-id\", StringType(), True),\n",
    "    StructField(\"dstaddr\", StringType(), True),\n",
    "    StructField(\"dstport\", LongType(), True),\n",
    "    StructField(\"srcaddr\", StringType(), True),\n",
    "    StructField(\"srcport\", LongType(), True),\n",
    "    StructField(\"start\", LongType(), True),\n",
    "    StructField(\"end\", LongType(), True),\n",
    "    StructField(\"log-status\", StringType(), True),\n",
    "    StructField(\"packets\", LongType(), True),\n",
    "    StructField(\"protocol\", LongType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"subnet-id\", StringType(), True),\n",
    "    StructField(\"tcp-flags\", LongType(), True),\n",
    "    StructField(\"traffic-path\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"vpc-id\", StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark.readStream.schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"s3a://cdp-dev-env-dq/spark-streaming/AWSLogs/621074188511/vpcflowlogs/ap-southeast-1/*/*/*/\", schema=schema, header = True, sep = ' ')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col, to_date, to_timestamp, date_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_dict = {\n",
    "    '1': 'ICMP',\n",
    "    '2': 'IGMP', \n",
    "    '3': 'GGP',\n",
    "    '4': 'IP-in-IP',\n",
    "    '6': 'TCP',\n",
    "    '8': 'EGP',\n",
    "    '9': 'IGP',\n",
    "    '17': 'UDP',\n",
    "    '41': 'IPv6',\n",
    "    '47': 'GRE',\n",
    "    '50': 'ESP',\n",
    "    '51': 'AH',\n",
    "    '58': 'ICMPv6',\n",
    "    '88': 'EIGRP',\n",
    "    '89': 'OSPF',\n",
    "    '92': 'MSP',\n",
    "    '103': 'PIM',\n",
    "    '112': 'VRRP',\n",
    "    '115': 'L2TP',\n",
    "    '132': 'SCTP',\n",
    "    '136': 'UDPLite'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, floor\n",
    "\n",
    "cleaned_streaming = streaming.withColumn(\"start\", from_unixtime(\"start\").cast(\"timestamp\")) \\\n",
    "                             .withColumn(\"end\", from_unixtime(\"end\").cast(\"timestamp\")) \\\n",
    "                             .withColumn(\"protocol\", when(col(\"protocol\") == \"1\", \"ICMP\")\n",
    "                                         .when(col(\"protocol\") == \"2\", \"IGMP\")\n",
    "                                         .when(col(\"protocol\") == \"3\", \"GGP\")\n",
    "                                         .when(col(\"protocol\") == \"4\", \"IP-in-IP\")\n",
    "                                         .when(col(\"protocol\") == \"6\", \"TCP\")\n",
    "                                         .when(col(\"protocol\") == \"8\", \"EGP\")\n",
    "                                         .when(col(\"protocol\") == \"9\", \"IGP\")\n",
    "                                         .when(col(\"protocol\") == \"17\", \"UDP\")\n",
    "                                         .when(col(\"protocol\") == \"41\", \"IPv6\")\n",
    "                                         .when(col(\"protocol\") == \"47\", \"GRE\")\n",
    "                                         .when(col(\"protocol\") == \"50\", \"ESP\")\n",
    "                                         .when(col(\"protocol\") == \"51\", \"AH\")\n",
    "                                         .when(col(\"protocol\") == \"58\", \"ICMPv6\")\n",
    "                                         .when(col(\"protocol\") == \"88\", \"EIGRP\")\n",
    "                                         .when(col(\"protocol\") == \"89\", \"OSPF\")\n",
    "                                         .when(col(\"protocol\") == \"92\", \"MSP\")\n",
    "                                         .when(col(\"protocol\") == \"103\", \"PIM\")\n",
    "                                         .when(col(\"protocol\") == \"112\", \"VRRP\")\n",
    "                                         .when(col(\"protocol\") == \"115\", \"L2TP\")\n",
    "                                         .when(col(\"protocol\") == \"132\", \"SCTP\")\n",
    "                                         .when(col(\"protocol\") == \"136\", \"UDPLite\")\n",
    "                                         .otherwise(\"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, split, substring\n",
    "\n",
    "# Create the boolean conditions\n",
    "accepted_idx = (col(\"action\") == \"ACCEPT\")\n",
    "rejected_idx = (col(\"action\") == \"REJECT\")\n",
    "\n",
    "dst_ip = split(col(\"dstaddr\"), \"\\\\.\")\n",
    "src_ip = split(col(\"srcaddr\"), \"\\\\.\")\n",
    "dst_idx = (substring(col(\"dstaddr\"), 1, 4) == \"10.0\")\n",
    "src_idx = (substring(col(\"srcaddr\"), 1, 4) == \"10.0\")\n",
    "\n",
    "in_idx = ~dst_idx & src_idx\n",
    "out_idx = ~src_idx & dst_idx\n",
    "local_idx = src_idx & dst_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the boolean conditions to the DataFrame|\n",
    "indexed_streaming = cleaned_streaming.withColumn(\"accepted_idx\", accepted_idx) \\\n",
    "                                    .withColumn(\"rejected_idx\", rejected_idx) \\\n",
    "                                    .withColumn(\"out_idx\", out_idx) \\\n",
    "                                    .withColumn(\"in_idx\", in_idx) \\\n",
    "                                    .withColumn(\"local_idx\", local_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the streaming DataFrame with a watermark\n",
    "local_traffic = indexed_streaming.filter(indexed_streaming.local_idx == True) \\\n",
    "                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                .groupBy(\"start\") \\\n",
    "                .count()\n",
    "\n",
    "accepted_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.accepted_idx == True) \\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "rejected_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.rejected_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "accepted_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "\n",
    "rejected_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.rejected_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('start').count()\n",
    "\n",
    "top10_in_traffic = indexed_streaming.filter(indexed_streaming.in_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('srcaddr', 'start').count()\n",
    "top10_out_traffic = indexed_streaming.filter(indexed_streaming.out_idx == True).filter(indexed_streaming.accepted_idx == True)\\\n",
    "                                                .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                                .groupBy('srcaddr', 'start').count()\n",
    "\n",
    "protocol_stream = indexed_streaming.select('protocol', 'start')\\\n",
    "                                    .withWatermark(\"start\", \"1 minute\") \\\n",
    "                                    .groupBy('protocol', 'start').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Write the stream to PostgreSQL\n",
    "protocol_query = (\n",
    "    protocol_stream.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"protocol\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/protocol/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# protocol_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Write the stream to PostgreSQL\n",
    "local_query = (\n",
    "    local_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"local\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/local/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Write the stream to PostgreSQL\n",
    "rejected_in_query = (\n",
    "    rejected_in_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"rejected_in\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/rejected_in/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Write the stream to PostgreSQL\n",
    "accepted_in_query = (\n",
    "    accepted_in_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"accepted_in\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/accepted_in/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the stream to PostgreSQL\n",
    "accepted_out_query = (\n",
    "    accepted_out_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"accepted_out\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/accepted_out/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the stream to PostgreSQL\n",
    "rejected_out_query = (\n",
    "    rejected_out_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"rejected_out\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/rejected_out/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Write the stream to PostgreSQL\n",
    "top10_in_query = (\n",
    "    top10_in_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"top10_in\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/top10_in/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Write the stream to PostgreSQL\n",
    "top10_out_query = (\n",
    "    top10_out_traffic.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"top10_out\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/top10_out/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# local_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the streaming DataFrame into a streaming DataFrame with two fields: type_of_packet and count\n",
    "packet_type_count = indexed_streaming \\\n",
    "    .withColumn(\"type_of_packet\", when(col(\"in_idx\") == \"true\", \"inbound\")\n",
    "                .when(col(\"out_idx\") == \"true\", \"outbound\")\n",
    "                .when(col(\"local_idx\") == \"true\", \"local\")\n",
    "                .otherwise(\"unknown\")) \\\n",
    "    .withWatermark(\"start\", \"1 minute\") \\\n",
    "    .groupBy(\"type_of_packet\", \"start\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .select(\"type_of_packet\", \"start\", 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 10:31:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the stream to PostgreSQL\n",
    "packet_type_query = (\n",
    "    packet_type_count.writeStream\n",
    "    .format(\"foreach\")\n",
    "    .foreachBatch(lambda df, id: write_to_postgres(df, id, \"type\"))\n",
    "    .option(\"checkpointLocation\", \"streaming_s3/packet_type/checkpoint/\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "# local_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 132.0 in stage 151.0 (TID 13502) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 135.0 in stage 151.0 (TID 13505) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 133.0 in stage 151.0 (TID 13503) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 07:44:46 WARN TaskSetManager: Lost task 134.0 in stage 151.0 (TID 13504) (172.26.0.5 executor 0): TaskKilled (Stage cancelled: Job 75 cancelled part of cancelled job group a1ff71b8-384f-4525-996f-1a86e4c07287)\n",
      "24/06/07 10:05:19 ERROR TaskSchedulerImpl: Lost executor 0 on 172.26.0.5: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "top10_out_query.stop()\n",
    "top10_in_query.stop()\n",
    "packet_type_query.stop()\n",
    "rejected_out_query.stop()\n",
    "accepted_in_query.stop()\n",
    "accepted_out_query.stop()\n",
    "rejected_in_query.stop()\n",
    "local_query.stop()\n",
    "protocol_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'packet_type_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpacket_type_query\u001b[49m\u001b[38;5;241m.\u001b[39mstatus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'packet_type_query' is not defined"
     ]
    }
   ],
   "source": [
    "# packet_type_query.status['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

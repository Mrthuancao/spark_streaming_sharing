{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Vinh-Le\") \\\n",
    "        .config(\"fs.s3a.access.key\", \"AKIAZBGXIBDPZ2FJZYF5\") \\\n",
    "        .config(\"fs.s3a.secret.key\", \"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\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4') \\\n",
    "        .config(\"fs.s3a.endpoint\", \"s3.ap-southeast-1.amazonaws.com\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"dynamic\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", 1500)\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "\n",
    "df = spark.read.parquet(\"s3a://cdp-dev-env-dq/raw-data/customer_details/\") \n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
